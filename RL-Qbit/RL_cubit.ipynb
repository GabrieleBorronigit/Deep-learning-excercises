{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096ddbd0-944a-434e-99f3-cb1ab6149a5b",
   "metadata": {},
   "source": [
    "## controllo quantistico di un qubit tramite Reinforcement Learning\n",
    "\n",
    "**Obiettivo**: addestrare un agente di Reinforcement Learning affinché impari a manipolare un **qubit** utilizzando un **campo esterno 2D controllabile**.  \n",
    "L'agente dovrà imparare una sequenza di impulsi che evolvano lo stato del qubit da uno stato iniziale \\( $|\\psi_0\\rangle \\ $) a uno stato target \\( $|\\psi_{\\text{target}}\\rangle \\$), massimizzando l'**overlap finale**.\n",
    "\n",
    "###  Modello fisico\n",
    "\n",
    "Il qubit evolve secondo una Hamiltoniana del tipo:\n",
    "\n",
    "\n",
    "$H(t) = u_x(t) \\cdot \\sigma_x + u_y(t) \\cdot \\sigma_y$\n",
    "\n",
    "\n",
    "dove:\n",
    "- \\( $\\sigma_x, \\sigma_y $\\) sono le matrici di Pauli\n",
    "- \\($ u_x(t), u_y(t) \\$) sono i controlli (impulsi) sull'asse X e Y\n",
    "- Il controllo è aggiornato passo dopo passo dall'agente RL\n",
    "\n",
    "L'evoluzione temporale è descritta dalla dinamica unitaria:\n",
    "\n",
    "\n",
    "|$\\psi(t+\\Delta t)\\rangle = e^{-i H(t) \\Delta t} \\cdot |\\psi(t)\\rangle $\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcac4e-c302-437e-b2b6-81f0511ace6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import expm #to esponentiate matrix\n",
    "\n",
    "#define Pauli matrix\n",
    "sigma_x = np.array([[0, 1], [1, 0]], dtype=np.complex128)\n",
    "sigma_y = np.array([[0, -1j], [1j, 0]], dtype=np.complex128)\n",
    "#define hamiltonian\n",
    "def H(ux,uy):\n",
    "    return ux*sigma_x+uy*sigma_y\n",
    "    \n",
    "#define time evolution\n",
    "def Evolve(state,ux,uy, deltat):\n",
    "    U=expm(-1j*H(ux,uy)*deltat)\n",
    "    new_state=U@state\n",
    "    return new_state/np.linalg.norm(new_state)\n",
    "\n",
    "#define fidelity (reward function) as the overlap |<psit|psi'>|^2 (to be maximized)\n",
    "def Fidelity(target_state,state): \n",
    "    f= np.abs(np.vdot(target_state, state))**2\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2774eb",
   "metadata": {},
   "source": [
    "\n",
    "# Reinforcement learning\n",
    "\n",
    "il reinforcement learning è un paradigma di deep learning in cui un **agente** interagisce con un **ambiente**, che fornisce **ricompense** numeriche in funzione delle **azioni**. L'agente impara una strategia otimale per **massimizzare la ricompensa ricevuta dall'ambiente**.\n",
    "\n",
    "Un problema di Reinforcement Learning si modella come un **MDP** (Markov Decision Process),  \n",
    "in cui lo stato dell'ambiente è caratterizzato da un set di parametri:\n",
    "\n",
    "$$\n",
    "Environment = (S, A, R, P, \\gamma)\n",
    "$$\n",
    "\n",
    "- Spazio degli stati accessibili del sistema \\(S\\)  \n",
    "- Spazio delle possibili azioni da compiere\\(A\\) \n",
    "- Spazio delle ricompense \\(R(s,a)\\)   \n",
    "- Spazio delle probabilità di transizione, distribuzioni di probabilità di stati e azioni successivi dati i precedenti  \n",
    "- Fattore di sconto \\($\\gamma$ $\\in [0,1]$\\)  \n",
    "\n",
    "In breve, l'algoritmo è il seguente:\n",
    " 1. l'agente seleziona un'azione $a_t$ secondo la sua **policy**\n",
    " 2. l'ambiente conferisce una ricompensa $r_t=R(\\cdot|s_t,a_t)$ e genera lo stato successivo $s_{t+1}=P(\\cdot |s_t,a_t)$\n",
    " 3. l'agente riceve $r_t$ ed $s_t$ e ripete\n",
    "\n",
    "La policy è una funzione $\\pi: S \\rightarrow A$ che viene utilizzata per scegliere l'azione da compiere in ogni stato. L'obiettivo dell'agente è trovare la policy che massimizzi la **ricompensa cumulativa scontata** $\\sum_{t}\\gamma^t r_t$.\n",
    "La miglior policy $\\pi^*$ è quella che massimizza la ricompensa e che tiene conto della natura stocastica dell'ambiente e della policy stessa,ovvero quella che in media, produce la sequenza di azioni e transizioni che porta al massimo ritorno possibile nel lungo termine. Formalmente:\n",
    "$$\n",
    "\\pi^* = \\arg\\max_{\\pi} \\; \\mathbb{E} \\left[ \\sum_{t \\ge 0} \\gamma^t r_t \\;\\middle|\\; \\pi \\right]\n",
    "$$\n",
    "\n",
    "Il problema del trovare la miglior policy può essere ridotto introducento la ** Q-value function Q(s,a)**, che misura l'efficacia di una coppia stato-azione dati i precedenti, mediando sulla ricompensa cumulativa seguendo la policy scelta:\n",
    "$$\n",
    "Q^\\pi(s,a) \\;=\\; \\mathbb{E} \\!\\left[ \\sum_{t \\ge 0} \\gamma^t r_t \n",
    "\\;\\middle|\\; s_0 = s,\\, a_0 = a,\\, \\pi \\right]\n",
    "$$\n",
    "Segue che la Q-value function **ottimale $Q^*(s,a)=max_{\\pi} (Q^\\pi)$** è quella che massimizza la ricompensa attesa $ r+\\gamma Q^*(s',a')$, ovvero quella soddisfacente l'**equazione di Bellman**:\n",
    "$$\n",
    "Q^*(s,a) = \\mathbb{E}_{s' \\sim P(\\cdot|s,a)} \n",
    "\\!\\left[ r + \\gamma \\max_{a'} Q^*(s',a') \\;\\middle|\\; s,a \\right]\n",
    "$$\n",
    "\n",
    "La **policy ottimale**, dunque, corrispondere a prendere **la miglior azione possibile in ogni stato, come specificato da $Q^*$**\n",
    "\n",
    "Operativamente, il **value iteration algorithm** usa l'equazione di Bellman come un update iterativo, stimando la Q dell'iterazione corrente come valore di aspettazione della ricompensa immediata più la miglior Q dell'iterazione precedente, pesando sulla distribuzione degli stati successivi:\n",
    "$$\n",
    "Q_{i+1}(s,a) \\;=\\; \\mathbb{E}_{s' \\sim P(\\cdot \\mid s,a)} \n",
    "\\Big[ \\, r(s,a) + \\gamma \\max_{a'} Q_i(s',a') \\,\\Big]\n",
    "$$\n",
    "Ad ogni passo la stima di Q migliora, si può mostrare che:\n",
    "$$\n",
    "\\lim_{i \\to \\infty} Q_i(s,a) = Q^*(s,a)\n",
    "$$\n",
    "\n",
    "Nel Q-learning classico (task semplici), è possibile tabulare tutti i valori di Q per ogni coppia azione-stato. Quando il problema diventa più complesso, come nel caso del qbit, si utilizza il paradigma **Deep Q-learning** in cui la Q viene **approssimata da una rete neurale profonda** $Q(s,a,\\theta)=Q(s,a)$. Formalmente il problema del deep Q-learning si può formulare come segue:\n",
    "$$L_i(\\theta_i)\n",
    "= \\mathbb{E}_{(s,a)\\sim \\rho(\\cdot)} \\Big[\\big(y_i - Q(s,a;\\theta_i)\\big)^2\\Big],\n",
    "\n",
    "$$\n",
    "$$\n",
    "y_i\n",
    "= \\mathbb{E}_{s' \\sim \\varepsilon}\\!\\left[\\, r + \\gamma \\max_{a'} Q(s',a';\\theta_{i-1}) \\,\\middle|\\, s,a \\right].\n",
    "$$\n",
    "dove si vuole minimizzare la loss-function L.\n",
    "\n",
    "---\n",
    "# Prima parte: spazio delle azioni discreto\n",
    "\n",
    "Una prima realizzazione del progetto di controllo di qbit tramite reinforcement learning è stata realizzata nel caso più semplice, in cui l'agente può scegliere l'azione da compiere (scelta dell'impulso $(u_x,u_y)$) da un **set discreto di valori** nell'intervallo $[-1,1]$.\n",
    "\n",
    "L'ambiente custom è definito utilizzando la libreria gymnasium. \n",
    "\n",
    "Lo stato iniziale è fissato come: $\\psi_0=\\frac{1}{\\sqrt(2)}(|0\\rangle + |1\\rangle)$, mentre quello target è $\\psi_T=|1\\rangle$.\n",
    "\n",
    "il metodo step, ad ogni azione, fornisce una ricompensa $r=|\\langle \\psi|\\psi_T\\rangle|^2$ e termina l'episodio se è già sufficientemente alta.\n",
    "\n",
    "L'agente DQN in questo caso è una rete con 4 layer fully-connected, di cui 3 con attivazione relu e l'ultimo lineare, che fa da regressore.\n",
    "\n",
    "La policy scelta in questo caso è la **epsilon-greedy policy**, che favorisce l'esplorazione, definita come:\n",
    "$$\n",
    "\\pi(a \\mid s) =\n",
    "\\begin{cases}\n",
    "1 - \\varepsilon + \\dfrac{\\varepsilon}{|A|}, & \\text{se } a = \\arg\\max\\limits_{a'} Q(s,a'), \\\\[10pt]\n",
    "\\dfrac{\\varepsilon}{|A|}, & \\text{altrimenti.}\n",
    "\\end{cases}\n",
    "$$\n",
    "con |A|= cardinalità dello spazio delle azioni. Il codice implementa il campionamento da questa distribuzione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ad406-3796-4f45-bae9-758a1981b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a gymnasium ambient\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class Qbitenvironment_discrete(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    #define possible actions: discrete tuning of fields\n",
    "        self.u_vals=np.linspace(-1.0, 1.0, 5)\n",
    "        self.actions = [(ux, uy) for ux in self.u_vals for uy in self.u_vals]\n",
    "        self.action_space = spaces.Discrete(len(self.actions)) #discrete space for now\n",
    "     #   ⟨σx⟩, ⟨σy⟩, ⟨σz⟩\n",
    "        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "        #timestep\n",
    "        self.dt = 0.15  \n",
    "        self.sigma_x = np.array([[0, 1], [1, 0]], dtype=np.complex128)\n",
    "        self.sigma_y = np.array([[0, -1j], [1j, 0]], dtype=np.complex128)\n",
    "        self.sigma_z = np.array([[1, 0], [0, -1]], dtype=np.complex128)\n",
    "        self.psi_target = np.array([0.0, 1.0], dtype=np.complex128)  # es. |+⟩\n",
    "        self.max_steps = 50\n",
    "        self.reset()\n",
    "     \n",
    "    def reset(self, seed=None, options=None):\n",
    "        psi = np.random.randn(2) + 1j*np.random.randn(2) #random initial state\n",
    "        psi/=np.linalg.norm(psi)        \n",
    "        self.psi=psi\n",
    "        #self.psi = np.array([1.0/np.sqrt(2), 1.0/np.sqrt(2)], dtype=np.complex128)  \n",
    "\n",
    "        obs = self._get_observation()\n",
    "        self.current_step = 0\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        ux, uy = self.actions[action_idx]\n",
    "        self.psi = Evolve(self.psi, ux, uy, self.dt)\n",
    "        obs = self._get_observation()\n",
    "        reward = Fidelity(self.psi_target, self.psi)\n",
    "        self.current_step += 1\n",
    "        truncated=False\n",
    "        terminated=False\n",
    "\n",
    "        if reward > 0.97:\n",
    "            terminated=True\n",
    "\n",
    "        if (self.current_step >= self.max_steps):\n",
    "            truncated = True \n",
    "        return obs, reward, terminated, truncated, {}\n",
    "    \n",
    "\n",
    "    def _get_observation(self):\n",
    "        sx = np.vdot(self.psi, self.sigma_x @ self.psi).real\n",
    "        sy = np.vdot(self.psi, self.sigma_y @ self.psi).real\n",
    "        sz = np.vdot(self.psi, self.sigma_z @ self.psi).real\n",
    "        return np.array([sx, sy, sz], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_agent(obs_shape, actions):\n",
    "    model=tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=obs_shape),\n",
    "                                      tf.keras.layers.Dense(256, activation='relu'),\n",
    "                                      tf.keras.layers.Dense(256, activation='relu'),\n",
    "\n",
    "                                      tf.keras.layers.Dense(actions, activation='linear')])\n",
    "    return model\n",
    "    \n",
    "#now define policy (it is a guideline on how the agent will behave during training). i choose\n",
    "#epsilon-greedy, which performs a random action with probability epsilon, otherwise it performs\n",
    "#the action which maximises the espected reward Q(s,a))\n",
    "def epsilon_greedy_policy(state, actions, agent, epsilon=0.1):\n",
    "    if(np.random.rand()<=epsilon):\n",
    "        return np.random.choice(actions)#random action with Prob=epsilon\n",
    "    else:\n",
    "        q_values=agent.predict(state[np.newaxis], verbose=0)\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8bd831",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Nella prima versione, il training loop è stato definito senza l'ausilio di ulteriori librerie. Il training funzione come descritto sopra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c1f3e-e2c4-486f-afb5-4e849acf06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Qbitenvironment_discrete()\n",
    "obs_shape = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#agent = build_agent(obs_shape, actions)               # online\n",
    "agent = load_model(\"dqn_qubit_model_discrete_50steps_fixedrestartsovrapp.keras\")   \n",
    "\n",
    "target_agent = build_agent(obs_shape, actions)        # target \n",
    "agent.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
    "target_agent.set_weights(agent.get_weights())         \n",
    "\n",
    "episodes = 1\n",
    "update_target_every = 1\n",
    "memory = deque(maxlen=20000)\n",
    "batch_size = 10\n",
    "gamma = 0.99\n",
    "\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    episode_reward = 0.0\n",
    "    terminated = False\n",
    "    truncated  = False\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        action = epsilon_greedy_policy(state, actions, agent)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        memory.append((state, action, reward, next_state, terminated, truncated))\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if len(memory) >= batch_size:\n",
    "            minibatch = random.sample(memory, batch_size)\n",
    "            states_mb      = np.stack([mb[0] for mb in minibatch])\n",
    "            actions_mb     = np.array([mb[1] for mb in minibatch])\n",
    "            rewards_mb     = np.array([mb[2] for mb in minibatch])\n",
    "            next_states_mb = np.stack([mb[3] for mb in minibatch])\n",
    "            terminated_mb  = np.array([mb[4] for mb in minibatch], dtype=bool)\n",
    "            truncated_mb   = np.array([mb[5] for mb in minibatch], dtype=bool)\n",
    "\n",
    "            # current Q-values (online)\n",
    "            q_current = agent.predict(states_mb, verbose=0)             # (B, A)\n",
    "\n",
    "            # predicted Qs for next states (target)\n",
    "            next_q_all = target_agent.predict(next_states_mb, verbose=0)  # (B, A)\n",
    "            max_next_q = next_q_all.max(axis=1)                            # (B,)\n",
    "            done = (terminated_mb | truncated_mb).astype(np.float32)\n",
    "\n",
    "            # Bellman targets (reward is r if episode is finished, else it's predicted by best Q target)\n",
    "            q_target_vec = rewards_mb + gamma * max_next_q * (1.0 - done)\n",
    "            # update the q prediction for the selcted action only\n",
    "            targets = q_current.copy()\n",
    "            targets[np.arange(batch_size), actions_mb] = q_target_vec\n",
    "            # train online network\n",
    "            agent.fit(states_mb, targets, epochs=1, verbose=0)\n",
    "\n",
    "    # after episode's end\n",
    "    if (episode + 1) % update_target_every == 0:\n",
    "        target_agent.set_weights(agent.get_weights())\n",
    "\n",
    "    reward_history.append(episode_reward)\n",
    "    if episode >= 10:\n",
    "        print(f\"Ep {episode:4d} | AvgR(10)={np.mean(reward_history[-10:]):.3f}\")\n",
    "    else:\n",
    "        print(f\"Ep {episode:4d} | R={episode_reward:.3f}\")\n",
    "\n",
    "agent.save(\"dqn_qubit_model_discrete_50steps_fixedrestartsovrapp.keras\")\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43272a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "env = Qbitenvironment_discrete()\n",
    "agent = load_model(\"dqn_qubit_model_discrete_30steps_restartrandom.keras\")   \n",
    "\n",
    "\n",
    "n_test_episodes = 100\n",
    "\n",
    "\n",
    "all_actions_ux = []  \n",
    "all_actions_uy = []   \n",
    "rewards_per_step = [] \n",
    "episode_rewards = []  \n",
    "test_steps=[]\n",
    "trajs=[]\n",
    "episode_final_rewards=[]\n",
    "\n",
    "for ep in range(n_test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    ep_actions_ux = []\n",
    "    ep_actions_uy = []\n",
    "    ep_rewards = []\n",
    "    ep_traj=[]\n",
    "    step=0\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        # policy greedy (no epsilon in test)\n",
    "        q_values = agent.predict(state[np.newaxis], verbose=0)\n",
    "        action = int(np.argmax(q_values[0]))\n",
    "\n",
    "        ux, uy = env.actions[action]\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # salva dati\n",
    "        ep_actions_ux.append(ux)\n",
    "        ep_actions_uy.append(uy)\n",
    "        ep_rewards.append(reward)\n",
    "        total_reward += reward\n",
    "        step+=1\n",
    "        state = next_state\n",
    "        ep_traj.append(env._get_observation())\n",
    "    # fine episodio: append alle liste globali\n",
    "    all_actions_ux.append(ep_actions_ux)\n",
    "    all_actions_uy.append(ep_actions_uy)\n",
    "    rewards_per_step.append(ep_rewards)\n",
    "    episode_rewards.append(total_reward)\n",
    "    test_steps.append(step)\n",
    "    trajs.append(ep_traj)\n",
    "    episode_final_rewards.append(ep_rewards[-1])\n",
    "    print(f\"Ep {ep+1}/{n_test_episodes} | TotReward={total_reward:.3f} | final reward={ep_rewards[-1]:.3f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "def subplots(ax,x,y, title,xlabel,ylabel, label=None):\n",
    "    ax.plot(x,y, '-o', label=label)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if label is not None:\n",
    "        ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "def scatterplot(ax,x,y, title,xlabel,ylabel, label=None):\n",
    "    ax.scatter(x,y, label=label, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if label is not None:\n",
    "        ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3,2, figsize=(30,20))\n",
    "ax=ax.flatten()\n",
    "N=np.arange(n_test_episodes)\n",
    "scatterplot(ax[0],N, episode_rewards , \"total reward on test episodes\", \"episodes\", \"r\")\n",
    "subplots(ax[1],N, test_steps, \"steps on test episodes\", \"episodes\", \"nsteps\")\n",
    "subplots(ax[2], np.arange(test_steps[-1]), rewards_per_step[-1], \"reward in last episode\", \"#steps\", \"r\")\n",
    "subplots(ax[3],np.arange(test_steps[-1]),ep_actions_ux, \"agent walk on last test episode\", \"# steps\", \"field\", \"ux\" )\n",
    "subplots(ax[3],np.arange(test_steps[-1]),ep_actions_uy, \"agent walk on last test episode\", \"# steps\", \"field\", \"uy\" )\n",
    "\n",
    "for ep in range(n_test_episodes):\n",
    "\n",
    "    subplots(ax[4],all_actions_ux[ep],all_actions_uy[ep], \"agent trajectories\", \"ux\", \"uy\" )\n",
    "ax[4].axis(\"equal\")\n",
    "scatterplot(ax[5], N, episode_final_rewards, \"final reward per episode\", \"#episode\", \"r\")\n",
    "ax[5].set_ylim(0,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_bloch_from_concatenated(traj, ax, show_points=True, add_labels=False):\n",
    "\n",
    "\n",
    "    traj = np.asarray(traj)\n",
    "    if traj.ndim != 3 or traj.shape[2] != 3:\n",
    "        raise ValueError(\"traj deve avere shape (nepisodi, nsteps, 3)\")\n",
    "\n",
    "    nepisodi = traj.shape[0]\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "\n",
    "    for i in range(nepisodi):\n",
    "        seg = traj[i]\n",
    "        sx, sy, sz = seg.T\n",
    "        color = cmap(i % 20)\n",
    "        label = f\"Ep {i+1}\" if add_labels else None\n",
    "        ax.plot(sx, sy, sz, linewidth=2, color=color, label=label)\n",
    "        if show_points:\n",
    "            ax.scatter(sx, sy, sz, s=12, color=color)\n",
    "\n",
    "# prepara la sfera una volta\n",
    "fig = plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "u = np.linspace(0, 2*np.pi, 80)\n",
    "v = np.linspace(0, np.pi, 40)\n",
    "xs = np.outer(np.cos(u), np.sin(v))\n",
    "ys = np.outer(np.sin(u), np.sin(v))\n",
    "zs = np.outer(np.ones_like(u), np.cos(v))\n",
    "ax.plot_wireframe(xs, ys, zs, linewidth=0.3, alpha=0.3)\n",
    "\n",
    "ax.text(0, 0, 1.1, r\"$|0\\rangle$\", fontsize=14, ha='center')\n",
    "ax.text(0, 0, -1.2, r\"$|1\\rangle$\", fontsize=14, ha='center')\n",
    "ax.set_xlabel(r'$\\langle \\sigma_x \\rangle$')\n",
    "ax.set_ylabel(r'$\\langle \\sigma_y \\rangle$')\n",
    "ax.set_zlabel(r'$\\langle \\sigma_z \\rangle$')\n",
    "ax.set_xlim([-1,1]); ax.set_ylim([-1,1]); ax.set_zlim([-1,1])\n",
    "ax.set_box_aspect([1,1,1])\n",
    "ax.set_title(\"Trajectory on Bloch's sphere (test episodes)\")\n",
    "\n",
    "\n",
    "plot_bloch_from_concatenated(trajs[-10:], ax, show_points=True, add_labels=True)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c6fb18",
   "metadata": {},
   "source": [
    "---\n",
    "# Parte 2: Spazio delle azioni continuo\n",
    "\n",
    "In questa versione più raffinata, l'agente può scegliere il valore del prossimo impulso da un set continui di valori in $[-1,1]$. Questo cambiamento introduce una maggiore complessità nel training, e il modello DQN precedentemente adottato non è più adeguato: avendo un set infinito di azioni, diventa impossibile scegliere la successiva stimano la Q(s,a) per ogni possibile scelta.\n",
    "\n",
    "## Training con modello Soft Actor Critic (SAC)\n",
    "\n",
    "L'idea fondamentale del modello SAC è quella di utilizzare una **policy parametrica** $\\pi_\\theta(a|s)$, detta **actor**, che restituisce una distribuzione da cui campionare le azioni. Le azioni campionate vengono valutate da 2 reti neurali, i **critic**, che ne calcolano le Q-functions.\n",
    "L'obiettivo è **massimizzare la ricompensa attesa e l'entropia della policy**, in modo da garantire esplorazione in ogni fase. Formalmente, si massimizza:\n",
    "$$\n",
    "J(\\pi) = \\mathbb{E}\\!\\Bigg[ \\sum_{t=0}^\\infty \\gamma^t \\Big( r(s_t,a_t) + \\alpha \\,\\mathcal{H}(\\pi(\\cdot|s_t)) \\Big) \\Bigg],\n",
    "\\qquad\n",
    "\\mathcal{H}(\\pi(\\cdot|s)) = -\\mathbb{E}_{a\\sim\\pi}[ \\log \\pi(a|s) ].\n",
    "\n",
    "$$\n",
    "dove $\\alpha$ è il parametro, aggiornato iterativamente durante il training, che regola l'entropia (quanto esplorare).\n",
    "\n",
    "Durante il training, i critic calcolano il target di Bellman \"soft\":\n",
    "$$\n",
    "y(r,s') = r + \\gamma \\,\\mathbb{E}_{a' \\sim \\pi_\\theta(\\cdot|s')} \n",
    "\\Big[ \\min_i Q_{\\phi_i}(s',a') - \\alpha \\log \\pi_\\theta(a'|s') \\Big].\n",
    "\n",
    "$$\n",
    "Dove il min(Q) serve per ridurre l'everestimation bias\n",
    "\n",
    "e minimizzano MSE tra stima e target:\n",
    "$$\n",
    "L_Q(\\phi_i) = \\mathbb{E}_{(s,a)\\sim \\mathcal{D}}\n",
    "\\Big[ \\big(Q_{\\phi_i}(s,a) - y(r,s')\\big)^2 \\Big].\n",
    "$$\n",
    "\n",
    "Per quanto riguarda l'actor, deve produrre azioni con Q alto, ma che mantengano entropia. L'update della policy avviene come segue:\n",
    "$$\n",
    "J_\\pi(\\theta) = \\mathbb{E}_{s \\sim \\mathcal{D},\\, a \\sim \\pi_\\theta}\n",
    "\\Big[ \\alpha \\log \\pi_\\theta(a|s) - Q_\\phi(s,a) \\Big].\n",
    "$$\n",
    "Dopo ogni update si aggiorna il parametro di entropia, minimizzando la sua loss risetto a un valore target $\\mathcal{H}_{\\text{target}}$:\n",
    "$$\n",
    "L(\\alpha) = \\mathbb{E}_{a \\sim \\pi_\\theta}\n",
    "\\Big[ -\\alpha \\, (\\log \\pi_\\theta(a|s) + \\mathcal{H}_{\\text{target}}) \\Big].\n",
    "$$\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "1. la policy campiona un'azione dalla distribuzione (gaussiana) generata\n",
    "2. l'ambiente formisce reward e stato successivo\n",
    "3. i Critic si aggiornano minimizzando la MSE delle loro predizioni sullo stato corrente rispetto al target di Bellman soft\n",
    "4. la policy viene aggiornata per minimizzare J\n",
    "5. il parametro $\\alpha$ viene adattato minimizzandone la differenza rispetto a un target di eplorazione fissato\n",
    "\n",
    "La libreria stable-baselines3 possiede un implementazione completa ed efficiente dell'agente SAC, che è quella utilizzata di seguito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a14a82-fa48-4d1c-820c-55d1e0239e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parte 2: passo a spazio continuo\n",
    "#define a gymnasium ambient\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class Qbitenvironment_continuum(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    #define possible actions: continuous tuning of fields\n",
    "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "\n",
    "     # Stato osservabile: ⟨σx⟩, ⟨σy⟩, ⟨σz⟩\n",
    "        self.observation_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "\n",
    "        self.dt = 0.15  # passo temporale\n",
    "        \n",
    "        self.sigma_x = np.array([[0, 1], [1, 0]], dtype=np.complex128)\n",
    "        self.sigma_y = np.array([[0, -1j], [1j, 0]], dtype=np.complex128)\n",
    "        self.sigma_z = np.array([[1, 0], [0, -1]], dtype=np.complex128)\n",
    "\n",
    "        self.psi_target = np.array([0.0, 1.0], dtype=np.complex128)  # es. |+⟩\n",
    "        self.max_steps = 15\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        #self.psi = np.array([1.0, 0.0], dtype=np.complex128)  # |0⟩\n",
    "        #self.psi = np.array([1.0/np.sqrt(2), 1.0/np.sqrt(2.0)], dtype=np.complex128)  # quasi già in |+⟩\n",
    "        psi = np.random.randn(2) + 1j*np.random.randn(2) #random initial state\n",
    "        psi/=np.linalg.norm(psi)\n",
    "        self.psi = psi\n",
    "        obs = self._get_observation()\n",
    "        self.current_step = 0\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        ux = np.clip(action[0], -1.0, 1.0)\n",
    "        uy = np.clip(action[1], -1.0, 1.0)\n",
    "        self.psi = Evolve(self.psi, ux, uy, self.dt)\n",
    "        fidelity = Fidelity(self.psi_target, self.psi)\n",
    "        obs = self._get_observation()\n",
    "        self.current_step += 1\n",
    "        #reward = -np.log(1 - fidelity + 1e-6)  # diverge quando fidelity → 1\n",
    "        #reward=1/(1-fidelity+1e-6)\n",
    "        reward=fidelity\n",
    "        truncated=False\n",
    "        terminated=False\n",
    "       # print(f\"norm_psi: {np.linalg.norm(self.psi):.4f}, reward: {reward:.4f}, psi={self.psi}\")\n",
    "        #print(ux,uy)\n",
    "        if fidelity > 0.97:\n",
    "            terminated=True\n",
    "            \n",
    "        if (self.current_step >= self.max_steps):\n",
    "            truncated = True \n",
    "        return obs, reward, terminated, truncated, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        sx = np.vdot(self.psi, self.sigma_x @ self.psi).real\n",
    "        sy = np.vdot(self.psi, self.sigma_y @ self.psi).real\n",
    "        sz = np.vdot(self.psi, self.sigma_z @ self.psi).real\n",
    "        return np.array([sx, sy, sz], dtype=np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#sac by hand\n",
    "env = Qbitenvironment_continuum()\n",
    "obs_shape = env.observation_space.shape[0]\n",
    "actions_shape = env.action_space.shape[0]\n",
    "#define Q(s,a) networks\n",
    "def build_Q(obs_shape, actions_shape):\n",
    "     input_obs= tf.keras.layers.Input(shape=(obs_shape,))\n",
    "     input_actions= tf.keras.layers.Input(shape=(actions_shape,))\n",
    "     input=tf.keras.layers.Concatenate()([input_obs, input_actions])\n",
    "     h1=tf.keras.layers.Dense(256, activation='relu')(input)\n",
    "     h2=tf.keras.layers.Dense(256, activation='relu')(h1)\n",
    "     Q=tf.keras.layers.Dense(1, activation='linear')(h2)\n",
    "     model=tf.keras.Model(inputs=[input_obs, input_actions], outputs=Q)\n",
    "     return model\n",
    "\n",
    "#define policy network\n",
    "def build_policy(obs_shape, actions_shape):\n",
    "     inputs= tf.keras.layers.Input(shape=(obs_shape,))\n",
    "     h1= tf.keras.layers.Dense(256, activation='relu')(inputs)\n",
    "     h2=tf.keras.layers.Dense(256, activation='relu')(h1)\n",
    "     mu=tf.keras.layers.Dense(actions_shape, activation='linear')(h2)#first output layer\n",
    "     logsigma=tf.keras.layers.Dense(actions_shape, activation='linear')(h2)#second output layer\n",
    "     model=tf.keras.Model(inputs=inputs, outputs=[mu,logsigma])\n",
    "     return model\n",
    "\n",
    "def sample_from_policy(mu, log_sigma):\n",
    "    sigma = tf.exp(log_sigma)\n",
    "    eps = tf.random.normal(shape=tf.shape(mu))\n",
    "    a_prime = mu + sigma * eps\n",
    "    a = tf.tanh(a_prime)\n",
    "    #logπ(a)=logN(a′∣μ,σ2)−i∑​log(1−tanh2(ai′​)) (change of variables)\n",
    "    # log N(a'|mu, sigma^2)\n",
    "    log_prob_gauss = -0.5 * (\n",
    "        tf.square((a_prime - mu) / (sigma + 1e-8)) +\n",
    "        2.0 * log_sigma +\n",
    "        tf.math.log(2.0 * np.pi)\n",
    "    )\n",
    "    log_prob_gauss = tf.reduce_sum(log_prob_gauss, axis=-1, keepdims=True)\n",
    "    log_det_jac = tf.reduce_sum(\n",
    "        tf.math.log(1.0 - tf.square(tf.tanh(a_prime)) + 1e-6),\n",
    "        axis=-1, keepdims=True\n",
    "    )\n",
    "\n",
    "    log_pi = log_prob_gauss - log_det_jac\n",
    "    return a, log_pi\n",
    "\n",
    "#declaration of optimizers outside the function that calls them, otherwise they will reset their state whenever an opt step is called\n",
    "optimizer_q1 = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "optimizer_q2 = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "optimizer_pi = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "optimizer_alpha= tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "log_alpha = tf.Variable(0.0, trainable=True, dtype=tf.float32)#to guarantee alpha>=0\n",
    "alpha = tf.exp(log_alpha)\n",
    "\n",
    "\n",
    "def Train_Qs(Q1, Q2,state_mb, action_mb, targets):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "     #calculate predictions\n",
    "        predicted_q1s = Q1([state_mb, action_mb], training=True)\n",
    "        predicted_q2s = Q2([state_mb, action_mb], training=True)\n",
    "      #evaluate mse\n",
    "        mse1= tf.reduce_mean(tf.square(predicted_q1s-targets))\n",
    "        mse2= tf.reduce_mean(tf.square(predicted_q2s-targets))\n",
    "\n",
    "      #evaluate gradients\n",
    "    grad1= tape.gradient(mse1, Q1.trainable_variables)\n",
    "    grad2= tape.gradient(mse2, Q2.trainable_variables)\n",
    "      #perform one optimization step using adam\n",
    "    optimizer_q1.apply_gradients(zip(grad1, Q1.trainable_variables))#apply_gradients want a[gradient, weights]->zip\n",
    "    optimizer_q2.apply_gradients(zip(grad2, Q2.trainable_variables))\n",
    "    del tape\n",
    "    return mse1, mse2\n",
    "\n",
    "\n",
    "def Train_policy(state_mb, Q1, Q2, policy, log_alpha):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        mu, logsigma = policy(state_mb, training=True)\n",
    "        #evaluate action and logpi\n",
    "        action, log_pi = sample_from_policy(mu, logsigma)\n",
    "        #update temperature\n",
    "        entropy = Update_alpha( log_alpha, log_pi)\n",
    "        #evaluate best estimation of q for the sampled action\n",
    "        q1=Q1([state_mb, action], training=False)\n",
    "        q2=Q2([state_mb, action], training=False)\n",
    "        q=tf.minimum(q1,q2)\n",
    "        #evaluate loss\n",
    "        loss=-(tf.reduce_mean(q-tf.exp(log_alpha)*log_pi))#negative loss, i want to maximize -loss\n",
    "        #evaluate gradient\n",
    "    grad=tape.gradient(loss, policy.trainable_variables)\n",
    "        #perform optimization step\n",
    "    optimizer_pi.apply_gradients(zip(grad, policy.trainable_variables))\n",
    "    return loss, entropy\n",
    "    \n",
    "    \n",
    "def update_target_network(Q, target_Q, rho=0.995):\n",
    "    #rho is a parameter that handles the \"velocity\" of updating the target Qs with the main Qs parameters. \n",
    "    #e.g. rho=0-> copy all the parameters of Q into the target network. \n",
    "    #by doing so, the target network weights become a mobile average of main network's (stabilize bootstrap of Qs during training)\n",
    "    main_weights=Q.get_weights()\n",
    "    target_weights=target_Q.get_weights()\n",
    "    updated_target_weights= [rho * tw + (1 - rho) * mw for mw, tw in zip(main_weights, target_weights)]\n",
    "    target_Q.set_weights(updated_target_weights)\n",
    "\n",
    "\n",
    "def Update_alpha(log_alpha, logpi, Htarget=-2): #from the paper: Htarget=-dim(action space)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        alpha=tf.exp(log_alpha)\n",
    "        loss=-tf.reduce_mean(alpha*(logpi+Htarget))\n",
    "    grad =tape.gradient(loss, [log_alpha])\n",
    "    optimizer_alpha.apply_gradients(zip(grad, [log_alpha]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a72566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "#training loop\n",
    "#define two Q functions\n",
    "Q1=build_Q(obs_shape, actions_shape)\n",
    "Q2=build_Q(obs_shape, actions_shape)\n",
    "#define target copies\n",
    "target_Q1=build_Q(obs_shape, actions_shape)\n",
    "target_Q2=build_Q(obs_shape, actions_shape)\n",
    "target_Q1.set_weights(Q1.get_weights())\n",
    "target_Q2.set_weights(Q2.get_weights())\n",
    "#define policy\n",
    "policy=build_policy(obs_shape, actions_shape)\n",
    "#initialize replay buffer\n",
    "\"\"\"\n",
    "save_dir = \"sac_checkpoints.keras\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "policy = tf.keras.models.load_model(os.path.join(save_dir, \"policy_50steps_restartfixedto0.keras\"))\n",
    "Q1 = tf.keras.models.load_model(os.path.join(save_dir, \"Q1_50steps_restartfixedto0.keras\"))\n",
    "Q2 = tf.keras.models.load_model(os.path.join(save_dir, \"Q2_50steps_restartfixedto0.keras\"))\n",
    "target_Q1 = tf.keras.models.load_model(os.path.join(save_dir, \"target_Q1_50steps_restartfixedto0.keras\"))\n",
    "target_Q2 = tf.keras.models.load_model(os.path.join(save_dir, \"target_Q2_50steps_restartfixedto0.keras\"))\n",
    "\n",
    "# carica log_alpha\n",
    "log_alpha = tf.Variable(np.load(os.path.join(save_dir, \"log_alpha_50steps_restartfixedto0.npy\")),\n",
    "                        trainable=True, dtype=tf.float32)\n",
    "                     \"\"\"\n",
    "replay_buffer = deque(maxlen=200000)\n",
    "critic_losses1=[]\n",
    "critic_losses2=[]\n",
    "actor_losses=[]\n",
    "entropy_losses=[]\n",
    "reward_history=[]\n",
    "#training loop\n",
    "episodes = 8000\n",
    "update_target_every = 1\n",
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, info= env.reset()\n",
    "    terminated= False\n",
    "    truncated = False\n",
    "    episode_reward=0.0\n",
    "    steps=0\n",
    "    while not (terminated or truncated):\n",
    "        mu, logsigma=policy(state[np.newaxis], training=False)\n",
    "        action, _=sample_from_policy(mu,logsigma)\n",
    "        next_state, reward, terminated, truncated, _=env.step(action.numpy()[0])\n",
    "        done = terminated or truncated\n",
    "        replay_buffer.append((state, action, reward, next_state, done) )\n",
    "        state=next_state\n",
    "        episode_reward+=reward\n",
    "        steps+=1\n",
    "        #training\n",
    "        if(len(replay_buffer)>=batch_size):\n",
    "            #random sample a batch of transitions from replay buffer\n",
    "            minibatch= random.sample(replay_buffer, batch_size)\n",
    "            #targets yi:\n",
    "            state_mb, action_mb, reward_mb, next_state_mb, done_mb=map(np.array, zip(*minibatch))\n",
    "            reward_mb = tf.reshape(tf.convert_to_tensor(reward_mb, dtype=tf.float32), (-1,1))\n",
    "            done_mb   = tf.reshape(tf.convert_to_tensor(done_mb, dtype=tf.float32), (-1,1))\n",
    "            state_mb  = tf.convert_to_tensor(state_mb, dtype=tf.float32)\n",
    "            action_mb = tf.convert_to_tensor(action_mb, dtype=tf.float32)\n",
    "            action_mb = tf.squeeze(action_mb, axis=1)  # (64,1,2) → (64,2)\n",
    "\n",
    "            next_state_mb = tf.convert_to_tensor(next_state_mb, dtype=tf.float32)\n",
    "\n",
    "            #evaluate target Qs predictions and take the min\n",
    "            mutrial, logsigmatrial = policy(next_state_mb, training= False)\n",
    "            trial_action, log_pi = sample_from_policy(mutrial, logsigmatrial)\n",
    "            target_qs1=target_Q1([next_state_mb, trial_action], training= False)\n",
    "            target_qs2=target_Q2([next_state_mb, trial_action], training=False)\n",
    "            #evaluate targets\n",
    "            y=reward_mb+gamma*(1-done_mb)*(tf.minimum(target_qs1, target_qs2)- tf.exp(log_alpha)*log_pi)\n",
    "            #update Q networks\n",
    "            mse1, mse2= Train_Qs(Q1, Q2, state_mb, action_mb, y)\n",
    "            critic_losses1.append(mse1)\n",
    "            critic_losses2.append(mse2)\n",
    "            #update policy network (and temperature)\n",
    "            actor_loss, entropy_loss=Train_policy(state_mb, Q1, Q2, policy, log_alpha)\n",
    "            actor_losses.append(actor_loss)\n",
    "            entropy_losses.append(entropy_loss)\n",
    "            #update target networks\n",
    "            if(steps)%update_target_every==0:\n",
    "                update_target_network(Q1, target_Q1)\n",
    "                update_target_network(Q2, target_Q2)\n",
    " \n",
    "\n",
    "    reward_history.append(episode_reward)\n",
    "    if episode >= 10:\n",
    "        print(f\"Ep {episode:4d} | R={episode_reward:.3f}| AvgR(10)={np.mean(reward_history[-10:]):.3f}| steps={steps}\")\n",
    "    else:\n",
    "        print(f\"Ep {episode:4d} | R={episode_reward:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# salvataggio modelli\n",
    "save_dir = \"sac_checkpoints.keras\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "policy.save(os.path.join(save_dir, \"policy_50steps_restartfixedto0_final.keras\"))\n",
    "Q1.save(os.path.join(save_dir, \"Q1_50steps_restartfixedto0_final.keras\"))\n",
    "Q2.save(os.path.join(save_dir, \"Q2_50steps_restartfixedto0_final.keras\"))\n",
    "target_Q1.save(os.path.join(save_dir, \"target_Q1_50steps_restartfixedto0_final.keras\"))\n",
    "target_Q2.save(os.path.join(save_dir, \"target_Q2_50steps_restartfixedto0_final.keras\"))\n",
    "\n",
    "# salvataggio log_alpha (variabile standalone)\n",
    "np.save(os.path.join(save_dir, \"log_alpha_50steps_restartfixedto0_final.npy\"), log_alpha.numpy())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa94134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carica modelli\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "save_dir = \"sac_checkpoints.keras\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "policy = tf.keras.models.load_model(os.path.join(save_dir, \"policy_15steps_restartrandom.keras\"))\n",
    "\n",
    "\n",
    "\n",
    "def sample_from_policy_eval(mu, log_sigma, eps=1e-6):\n",
    "    sigma = tf.exp(tf.clip_by_value(log_sigma, -5.0, 2.0))\n",
    "    epsn  = tf.random.normal(shape=tf.shape(mu))\n",
    "    a_p   = mu + sigma * epsn\n",
    "    a     = tf.tanh(a_p)\n",
    "    return a\n",
    "\n",
    "\n",
    "total_rewards = []\n",
    "n_test_episodes = 50\n",
    "\n",
    "\n",
    "all_actions_ux = []  \n",
    "all_actions_uy = []   \n",
    "rewards_per_step = []\n",
    "episode_rewards = []  \n",
    "test_steps=[]\n",
    "trajs=[]\n",
    "episode_final_rewards=[]\n",
    "for ep in range(n_test_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "    step = 0\n",
    "    ep_actions_ux = []\n",
    "    ep_actions_uy = []\n",
    "    ep_rewards = []\n",
    "    ep_traj=[]\n",
    "    total_reward=0.0\n",
    "\n",
    "    while not done:\n",
    "        mu, logsigma = policy(state[np.newaxis], training=False)\n",
    "        action = sample_from_policy_eval(mu, logsigma).numpy()[0]\n",
    "        ux=action[0]\n",
    "        uy=action[1]\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ep_actions_ux.append(ux)\n",
    "        ep_actions_uy.append(uy)\n",
    "        ep_rewards.append(reward)\n",
    "        total_reward += reward\n",
    "        step+=1\n",
    "        state = next_state\n",
    "        ep_traj.append(env._get_observation())\n",
    "\n",
    "\n",
    "    all_actions_ux.append(ep_actions_ux)\n",
    "    all_actions_uy.append(ep_actions_uy)\n",
    "    rewards_per_step.append(ep_rewards)\n",
    "    episode_rewards.append(total_reward)\n",
    "    test_steps.append(step)\n",
    "    trajs.append(ep_traj)\n",
    "    episode_final_rewards.append(ep_rewards[-1])\n",
    "\n",
    "    \n",
    "    print(f\"Ep {ep+1}/{n_test_episodes} | TotReward={total_reward:.3f}\")\n",
    "    total_rewards.append(ep_reward)\n",
    "    print(f\"[TEST] Ep {ep+1}/{n_test_episodes} | R={ep_reward:.3f} final reward={ep_rewards[-1]:.3f}| steps={step}\")\n",
    "avgR = np.mean(total_rewards)\n",
    "print(f\"[TEST] AvgR over {n_test_episodes} episodes = {avgR:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def subplots(ax,x,y, title,xlabel,ylabel, label=None):\n",
    "    ax.plot(x,y, '-o', label=label)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if label is not None:\n",
    "        ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "def scatterplot(ax,x,y, title,xlabel,ylabel, label=None):\n",
    "    ax.scatter(x,y, label=label, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if label is not None:\n",
    "        ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3,4, figsize=(30,20))\n",
    "ax=ax.flatten()\n",
    "N=np.arange(n_test_episodes)\n",
    "scatterplot(ax[0],N, episode_rewards , \"total reward on test episodes\", \"episodes\", \"r\")\n",
    "subplots(ax[1],N, test_steps, \"steps on test episodes\", \"episodes\", \"nsteps\")\n",
    "subplots(ax[2], np.arange(test_steps[-1]), rewards_per_step[-1], \"reward in last episode\", \"#steps\", \"r\")\n",
    "subplots(ax[3],np.arange(test_steps[-1]),ep_actions_ux, \"agent walk on last test episode\", \"# steps\", \"field\", \"ux\" )\n",
    "subplots(ax[3],np.arange(test_steps[-1]),ep_actions_uy, \"agent walk on last test episode\", \"# steps\", \"field\", \"uy\" )\n",
    "\n",
    "for ep in range(n_test_episodes):\n",
    "\n",
    "    subplots(ax[4],all_actions_ux[ep],all_actions_uy[ep], \"agent trajectories\", \"ux\", \"uy\" )\n",
    "ax[4].axis(\"equal\")\n",
    "scatterplot(ax[5], N, episode_final_rewards, \"final reward per episode\", \"#episode\", \"r\")\n",
    "ax[5].set_ylim(0,1)\n",
    "subplots(ax[6], np.arange(len(actor_losses)), actor_losses, \"actor loss during training\", \"episode\", \"Loss\")\n",
    "subplots(ax[7], np.arange(len(entropy_losses)), entropy_losses, \"entropy loss during training\", \"episode\", \"Loss\")\n",
    "scatterplot(ax[8], np.arange(len(critic_losses1)), critic_losses1, \"critic loss during training\", \"episode\", \"Loss\", \"Q1\")\n",
    "scatterplot(ax[8], np.arange(len(critic_losses2)), critic_losses2, \"critic loss during training\", \"episode\", \"Loss\", \"Q2\")\n",
    "scatterplot(ax[10], np.arange(len(reward_history)), reward_history, \"reward during training\", \"episode\", \"reward\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea40fc0-2966-4f9f-91da-46a760a57130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = Qbitenvironment_continuum()\n",
    "check_env(env)  # controlla correttezza spazi\n",
    "\n",
    "policy_kwargs = dict(net_arch=[256, 256])\n",
    "history = HistoryCallback()\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_starts=10_000,\n",
    "    buffer_size=200_000,\n",
    "    batch_size=128,\n",
    "    learning_rate=3e-4,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    tau=0.005,\n",
    "    ent_coef=\"auto\",\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    \n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=2_000_000, callback=history)\n",
    "model.save(\"sac_qubit_15steps_randominit\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
